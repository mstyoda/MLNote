\begin{thebibliography}{1}

\bibitem{dinh2017sharp}
{\sc Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.}
\newblock Sharp minima can generalize for deep nets.
\newblock {\em arXiv preprint arXiv:1703.04933\/} (2017).

\bibitem{hardt2015train}
{\sc Hardt, M., Recht, B., and Singer, Y.}
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock {\em arXiv preprint arXiv:1509.01240\/} (2015).

\bibitem{keskar2016large}
{\sc Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P.
  T.~P.}
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em arXiv preprint arXiv:1609.04836\/} (2016).

\bibitem{zhang2016understanding}
{\sc Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.}
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530\/} (2016).

\end{thebibliography}
